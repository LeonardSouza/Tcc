{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonardSouza/Tcc/blob/main/ClassificadorDeImagem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qARTsXpKtulF"
      },
      "source": [
        "# Smart Image Sorter üñºÔ∏èüìÅ\n",
        "\n",
        "<a href=\"https://www.bellingcat.com\"><img alt=\"Bellingcat logo: Discover Bellingcat\" src=\"https://img.shields.io/badge/Discover%20Bellingcat-%20?style=for-the-badge&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAA4AAAAYCAYAAADKx8xXAAABhGlDQ1BJQ0MgcHJvZmlsZQAAKJF9kT1Iw0AcxV9TS0UqDnZQEcxQneyiIo6likWwUNoKrTqYXPoFTRqSFBdHwbXg4Mdi1cHFWVcHV0EQ%2FABxdnBSdJES%2F5cUWsR4cNyPd%2Fced%2B8AoVllqtkTA1TNMtKJuJjLr4rBVwQwhhBEDEvM1JOZxSw8x9c9fHy9i%2FIs73N%2Fjn6lYDLAJxLHmG5YxBvEs5uWznmfOMzKkkJ8Tjxp0AWJH7kuu%2FzGueSwwDPDRjY9TxwmFktdLHcxKxsq8QxxRFE1yhdyLiuctzir1Tpr35O%2FMFTQVjJcpzmKBJaQRIo6klFHBVVYiNKqkWIiTftxD%2F%2BI40%2BRSyZXBYwcC6hBheT4wf%2Fgd7dmcXrKTQrFgcCLbX%2BMA8FdoNWw7e9j226dAP5n4Err%2BGtNYO6T9EZHixwBA9vAxXVHk%2FeAyx1g6EmXDMmR%2FDSFYhF4P6NvygODt0Dfmttbex%2BnD0CWulq%2BAQ4OgYkSZa97vLu3u7d%2Fz7T7%2BwHEU3LHAa%2FQ6gAAAAZiS0dEAAAAAAAA%2BUO7fwAAAAlwSFlzAAAuIwAALiMBeKU%2FdgAAAAd0SU1FB%2BgFHwwiMH4odB4AAAAZdEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIEdJTVBXgQ4XAAAA50lEQVQ4y82SvWpCQRCFz25ERSJiCNqlUiS1b5AuEEiZIq1NOsGXCKms0wXSp9T6dskDiFikyiPc%2FrMZyf3FXSGQ0%2BzuzPl2ZoeVKgQ0gQ2wBVpVHlcDkjM5V%2FJ5nag6sJ%2FZX%2Bh%2FC7gEhqeAFKf7p1M9aB3b5oN1OomB7g1axUBPBr3GQHODHmOgqUF3MZAzKI2d4LWBV4H%2BMXDuJd1a7Cew1k7SwksaHC4LqNaw7aeX9GWHXkC1G1sTAS17Y3Kk2lnp4wNLiz0DrgLq8qt2MfmSSabAO%2FBBXp26dtrADPjOmN%2BAUdG7B3cE61l5hOZiAAAAAElFTkSuQmCC&logoColor=%23fff&color=%23000\"></a><!--\n",
        "--><a href=\"https://discord.gg/bellingcat\"><img alt=\"Discord logo: Join our community\" src=\"https://img.shields.io/badge/Join%20our%20community-%20?style=for-the-badge&logo=discord&logoColor=%23fff&color=%235865F2\"></a><!--\n",
        "--><a href=\"https://www.bellingcat.com/resources/how-tos/2024/08/15/easy-ai-zero-shot-ai-image-classification-smart-image-sorter/\"><img alt=\"Book icon: read the article\" src=\"https://img.shields.io/badge/Explore%20the%20guide-%20?style=for-the-badge&logo=ReadMe&logoColor=fff&color=C41E3D\"></a>\n",
        "\n",
        "The Smart Image Sorter organises images into folders based on the classification results of an open-source AI model. The tool creates a subdirectory for each label and organises the images according to the label with the highest confidence score.\n",
        "\n",
        "This tool was developed by Adriano Belisario as part of the Bellingcat Tech Fellowship.\n",
        "\n",
        "<details>\n",
        "<summary>Click here to learn more about the settings you can configure. üí°</summary>\n",
        "\n",
        "**Mount Google Drive**\n",
        "\n",
        "If you are running this notebook using Google Colab, we recommend you click on this button.\n",
        "\n",
        "By default, Google Colab files are temporary, meaning they disappear when your session ends, either after a certain amount of time without any activity, or if you leave this page. Clicking on this button allows you to use your own images from Google Drive and save the sorted images easily.\n",
        "\n",
        "You may ignore this button if you are running this notebook locally.\n",
        "\n",
        "**Source**\n",
        "\n",
        "The path to the folder with the images to be classified. In Google Colab, you can use the file manager in the left menu (![folder.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACcAAAAkCAYAAAAKNyObAAAABHNCSVQICAgIfAhkiAAAARpJREFUWEftmDEOgzAMRZ2qS5hZ2ZlZOQkzh+MkrLAyw8oMY9sfiSUN2KSthCr/pSI4zvfDDRHm8RJdVLeL+nK21Fzs01FySi6WQOw87bm/JHfnqlqWhdq2pb7vg6FZllFVVcF7nw6y5pqmceb2NAwDoYC6rvdCosdZc13Xsclhfl1XAsUjJUlCZVkSfiVizWFRiVCEpJBxHMWUWXObMVScpqnEZzBmnufD3g1NOmUuz/NQDtEYehOPH/0plW7CUlJ+nJLziUivlZyUlB+n5Hwi0mt2E7bWuvfmNE1kjJHmfYvDfAj5pGLNFUXhdnacTr4h5JPKcJ8jtvOc5KXOLQpjZ04lrDluwV/e139rLF0lp+RiCcTOu3TPPQH7BVy6Xmp3EgAAAABJRU5ErkJggg==)) to get the path of your image folder. Right-click the folder, select \"Copy path\" and paste the directory below. The directories will be created if they do not exist.\n",
        "\n",
        "**Destination**\n",
        "\n",
        "The folder to which the images should be copied or moved after the classification. The code also generates an `output.csv` file in the destination folder, containing a table with all the filenames, labels and confidence score. If you are running this notebook using Google Colab, we recommend that you mount your Google Drive and use a destination file path from there to ensure your output is saved beyond your Google Colab session.\n",
        "\n",
        "**Model**\n",
        "\n",
        "The name of the zero-shot image classification model on HuggingFace that will be used.\n",
        "\n",
        "The default option is the most downloaded zero-shot image classification model on HuggingFace. [Check out other models available](https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&sort=downloads).\n",
        "\n",
        "If you want to geolocate images, try **StreetCLIP** ([`geolocal/StreetCLIP`](https://huggingface.co/geolocal/StreetCLIP)) and use countries, regions, or cities as candidate labels.\n",
        "\n",
        "**Labels**\n",
        "\n",
        "A comma-separated list of categories that the model will use for classification. Short, descriptive labels in English tend to work the best in most cases: e.g. \"a picture of a person\" rather than \"people\". The model will always assign one (and only one) of the labels to the image, even if none of them are appropriate, so your labels should be as comprehensive as possible and should not overlap.\n",
        "\n",
        "**Operation**\n",
        "\n",
        "Define if the tool should copy or move the images from the source to the destination folder when sorting them. ‚ÄúCopy‚Äù ensures that you retain the original unsorted images, while ‚Äúmove‚Äù may be more efficient if you are concerned about storage space.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "IztKkKdwXGpq",
        "outputId": "88af40cc-fdc3-4405-8af0-40abea89d279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233,
          "referenced_widgets": [
            "dbabf7257d0e4ec48060baed10d547cb",
            "f172636cffe74c0cb0188dbbb493bcd8",
            "84bcc5e0269b41fdbc47fc64a71686c0",
            "a785188fca254233a61a977c4799ae44",
            "e8cd7fde196746bbbd1d94ad3058416d",
            "35d9b56177ce4d8f9878a73a5416fd8a",
            "1d222d7020fb4d8593d0af4e1667592f",
            "5e18a50b5dd54d67add17d0ffaf7864a",
            "3209ea0eb336406183e2b436a34fdfab",
            "906447f676624559a402a9511222a25f",
            "d06c437aa9b34fb7887191dd225687d2",
            "fad999be0e8c42c6bd97b89bcfbbbf1e",
            "078ede06f0dc42279e8f6b9ee91b305f",
            "fbb7ce25ad4446149c43ab785d975bb3",
            "b6c21ed304d547019eb340475e9f87df",
            "7da18e4cc390436482e4d8d3ec05279c",
            "31eda5bd85da4074aa888440119cb963",
            "202790b5562f4ebbbe3ebfecb831f505",
            "5dd3b0dcca834bd2be30d7d817d8beff",
            "ae4823a5a2be49bb873d957d93a60054",
            "a8f3bea7ec56468f8ce3fc81137452bc"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='Mount Google Drive', icon='upload', style=ButtonStyle(), tooltip='‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbabf7257d0e4ec48060baed10d547cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='/content/smart-image-sorter/imgs', description='Source:', layout=Layout(width='50%'), placeholder=‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a785188fca254233a61a977c4799ae44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='labelled/', description='Destination:', layout=Layout(width='50%'), placeholder='Enter destination‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d222d7020fb4d8593d0af4e1667592f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Combobox(value='openai/clip-vit-large-patch14', description='Model:', ensure_option=True, options=('openai/cli‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "906447f676624559a402a9511222a25f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='screenshot of a newspaper article, a picture of a person, a picture of a politician, memes and com‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "078ede06f0dc42279e8f6b9ee91b305f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Operation:', options=('copy', 'move'), value='copy')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7da18e4cc390436482e4d8d3ec05279c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='Start Classification', icon='play', style=ButtonStyle(), tooltip='‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5dd3b0dcca834bd2be30d7d817d8beff"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Set up the parameters below\n",
        "\n",
        "# @markdown ### ‚¨ÖÔ∏è Click to load the tool\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "import os, sys, time\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def is_running_in_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "def clone_repository(repository_url, directory):\n",
        "    import subprocess\n",
        "    result = subprocess.run([\"git\", \"clone\", repository_url, directory], capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"\\x1b[31m There was an error while loading the tool: {result.stderr}\\x1b[0m\")\n",
        "\n",
        "\n",
        "def create_mount_google_drive_button():\n",
        "    # Create mount Google Drive button\n",
        "    mount_button = widgets.Button(\n",
        "                description='Mount Google Drive',\n",
        "                disabled=False,\n",
        "                button_style='success',\n",
        "                tooltip='Click to mount Google Drive',\n",
        "                icon='upload'\n",
        "            )\n",
        "\n",
        "    def on_mount_drive(b):\n",
        "        from google.colab import drive\n",
        "        GDRIVE_PATH = \"/content/drive/\"\n",
        "        drive.mount(GDRIVE_PATH)\n",
        "        print(f\"Google Drive mounted at {GDRIVE_PATH}\")\n",
        "\n",
        "    mount_button.on_click(on_mount_drive)\n",
        "    return mount_button\n",
        "\n",
        "\n",
        "def setup_notebook(gh_name, repo_name):\n",
        "    if is_running_in_colab():\n",
        "        # Check for GPU\n",
        "        if (os.environ.get(\"COLAB_GPU\")==''):\n",
        "            print(\"\\x1b[33m It looks like there isn't a GPU available, which could make the tool run very slowly. Try selecting a runtime with a GPU (Runtime > Change runtime type). \\x1b[0m\")\n",
        "\n",
        "        # Clone Repo\n",
        "        repo_url = f\"https://github.com/{gh_name}/{repo_name}.git\"\n",
        "\n",
        "        if not os.path.exists(repo_name):\n",
        "            clone_repository(repo_url, repo_name)\n",
        "\n",
        "        repo_path = os.path.abspath(repo_name)\n",
        "\n",
        "        if repo_path not in sys.path:\n",
        "            sys.path.append(repo_path)\n",
        "\n",
        "setup_notebook(\"bellingcat\", \"smart-image-sorter\")\n",
        "\n",
        "# GUI\n",
        "\n",
        "from utils.file_manager import setup_directories, list_images\n",
        "from utils.model import list_models, classify_images\n",
        "\n",
        "mount_button = create_mount_google_drive_button()\n",
        "\n",
        "all_models = list_models()\n",
        "\n",
        "labels_input = widgets.Text(\n",
        "    value=\"screenshot of a newspaper article, a picture of a person, a picture of a politician, memes and computer-generated images\",\n",
        "    placeholder='Enter comma-separated labels',\n",
        "    description='Labels:',\n",
        "    layout=widgets.Layout(width='50%'),\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "source_input = widgets.Text(\n",
        "    value=\"/content/smart-image-sorter/imgs\",\n",
        "    placeholder='Enter source directory path',\n",
        "    description='Source:',\n",
        "    layout=widgets.Layout(width='50%'),\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "destination_input = widgets.Text(\n",
        "    value=\"labelled/\",\n",
        "    placeholder='Enter destination directory path',\n",
        "    description='Destination:',\n",
        "    layout=widgets.Layout(width='50%'),\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "model_input = widgets.Combobox(\n",
        "    value=all_models[0],\n",
        "    placeholder=\"Select or type model name\",\n",
        "    options=all_models,\n",
        "    description='Model:',\n",
        "    ensure_option=True,\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "operation_input = widgets.Dropdown(\n",
        "    options=['copy', 'move'],\n",
        "    value='copy',\n",
        "    description='Operation:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "start_classification_button = widgets.Button(\n",
        "    description='Start Classification',\n",
        "    disabled=False,\n",
        "    button_style='success',\n",
        "    tooltip='Click to start image classification',\n",
        "    icon='play'\n",
        ")\n",
        "\n",
        "def on_start_classification(b):\n",
        "    clear_output()\n",
        "    display(source_input,destination_input,model_input,labels_input, operation_input,start_classification_button)\n",
        "    start_time = time.time()\n",
        "    source = source_input.value\n",
        "    destination = destination_input.value\n",
        "    setup_directories(source, destination)\n",
        "    labels = labels_input.value.split(',')\n",
        "    labels = [label.strip() for label in labels]\n",
        "    num_labels = len(labels)\n",
        "    if num_labels < 2:\n",
        "      print(f\"\\x1b[31m Please select two or more labels.\\x1b[0m\")\n",
        "    else:\n",
        "      print(f\"Labels: {labels}\")\n",
        "      print(f\"Source directory: {source}\")\n",
        "      print(f\"Destination directory: {destination}\")\n",
        "      print(f\"Starting classification...\")\n",
        "      images = list_images(source)\n",
        "      classify_images(images, destination, model_input.value, labels, operation_input.value)\n",
        "      print(f\"Classification finished\")\n",
        "      end_time = time.time()\n",
        "      elapsed_time = round(end_time - start_time)\n",
        "      print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "\n",
        "start_classification_button.on_click(on_start_classification)\n",
        "\n",
        "if is_running_in_colab():\n",
        "    display(mount_button)\n",
        "\n",
        "display(source_input,destination_input,model_input,labels_input, operation_input,start_classification_button)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dbabf7257d0e4ec48060baed10d547cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Mount Google Drive",
            "disabled": false,
            "icon": "upload",
            "layout": "IPY_MODEL_f172636cffe74c0cb0188dbbb493bcd8",
            "style": "IPY_MODEL_84bcc5e0269b41fdbc47fc64a71686c0",
            "tooltip": "Click to mount Google Drive"
          }
        },
        "f172636cffe74c0cb0188dbbb493bcd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84bcc5e0269b41fdbc47fc64a71686c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a785188fca254233a61a977c4799ae44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Source:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_e8cd7fde196746bbbd1d94ad3058416d",
            "placeholder": "Enter source directory path",
            "style": "IPY_MODEL_35d9b56177ce4d8f9878a73a5416fd8a",
            "value": "/content/smart-image-sorter/imgs"
          }
        },
        "e8cd7fde196746bbbd1d94ad3058416d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "35d9b56177ce4d8f9878a73a5416fd8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d222d7020fb4d8593d0af4e1667592f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Destination:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5e18a50b5dd54d67add17d0ffaf7864a",
            "placeholder": "Enter destination directory path",
            "style": "IPY_MODEL_3209ea0eb336406183e2b436a34fdfab",
            "value": "labelled/"
          }
        },
        "5e18a50b5dd54d67add17d0ffaf7864a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "3209ea0eb336406183e2b436a34fdfab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "906447f676624559a402a9511222a25f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ComboboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ComboboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ComboboxView",
            "continuous_update": true,
            "description": "Model:",
            "description_tooltip": null,
            "disabled": false,
            "ensure_option": true,
            "layout": "IPY_MODEL_d06c437aa9b34fb7887191dd225687d2",
            "options": [
              "openai/clip-vit-large-patch14",
              "openai/clip-vit-base-patch32",
              "google/siglip-so400m-patch14-384",
              "openai/clip-vit-base-patch16",
              "openai/clip-vit-large-patch14-336",
              "patrickjohncyh/fashion-clip",
              "laion/CLIP-ViT-B-32-laion2B-s34B-b79K",
              "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k",
              "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
              "laion/CLIP-ViT-B-16-laion2B-s34B-b88K",
              "laion/CLIP-convnext_base_w-laion2B-s13B-b82K-augreg",
              "yuvalkirstain/PickScore_v1",
              "google/siglip-base-patch16-224",
              "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup",
              "StanfordAIMI/XrayCLIP__vit-b-16__laion2b-s34b-b88k",
              "google/siglip2-so400m-patch14-384",
              "q-future/one-align",
              "timm/ViT-SO400M-14-SigLIP-384",
              "HuggingFaceM4/siglip-so400m-14-384",
              "Marqo/marqo-fashionSigLIP",
              "Xenova/clip-vit-base-patch32",
              "laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K",
              "laion/CLIP-ViT-L-14-laion2B-s32B-b82K",
              "kakaobrain/align-base",
              "imageomics/bioclip",
              "laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K",
              "apple/MobileCLIP-S2-OpenCLIP",
              "jinaai/jina-clip-v2",
              "laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K",
              "timm/ViT-SO400M-14-SigLIP",
              "timm/ViT-B-16-SigLIP-i18n-256",
              "timm/ViT-B-16-SigLIP",
              "facebook/metaclip-b32-400m",
              "laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg",
              "timm/eva02_base_patch16_clip_224.merged2b_s8b_b131k",
              "Marqo/marqo-fashionCLIP",
              "timm/vit_large_patch14_clip_336.openai",
              "microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224",
              "facebook/metaclip-h14-fullcc2.5b",
              "facebook/metaclip-b16-fullcc2.5b",
              "laion/CLIP-convnext_base_w-laion2B-s13B-b82K",
              "wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M",
              "LanguageBind/LanguageBind_Image",
              "google/siglip-base-patch16-512",
              "Xenova/clip-vit-base-patch16",
              "flaviagiammarino/pubmed-clip-vit-base-patch32",
              "timm/vit_base_patch16_plus_clip_240.laion400m_e31",
              "vinid/plip",
              "OFA-Sys/chinese-clip-vit-base-patch16",
              "LanguageBind/LanguageBind_Video_merge",
              "StanfordAIMI/XraySigLIP__vit-l-16-siglip-384__webli",
              "Salesforce/blip2-itm-vit-g",
              "timm/vit_large_patch14_clip_224.laion400m_e32",
              "laion/CLIP-ViT-g-14-laion2B-s34B-b88K",
              "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg",
              "LanguageBind/LanguageBind_Video_FT",
              "laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K",
              "google/siglip-large-patch16-384",
              "google/siglip2-large-patch16-256",
              "timm/ViT-B-16-SigLIP2-256",
              "timm/resnet50_clip.cc12m",
              "timm/vit_large_patch14_clip_224.laion400m_e31",
              "yujiepan/clip-vit-tiny-random-patch14-336",
              "timm/ViT-L-16-SigLIP-256",
              "timm/ViT-L-16-SigLIP-384",
              "timm/resnet101_clip.yfcc15m",
              "geolocal/StreetCLIP",
              "timm/vit_base_patch32_clip_224.laion400m_e32",
              "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup",
              "laion/CLIP-ViT-L-14-CommonPool.XL-s13B-b90K",
              "timm/vit_base_patch16_clip_224.laion400m_e32",
              "google/siglip-so400m-patch14-224",
              "timm/eva02_large_patch14_clip_224.merged2b_s4b_b131k",
              "google/siglip2-base-patch16-224",
              "google/siglip2-so400m-patch16-512",
              "google/siglip-base-patch16-256-multilingual",
              "wisdomik/QuiltNet-B-32",
              "timm/ViT-B-16-SigLIP-256",
              "timm/vit_base_patch32_clip_224.laion2b_e16",
              "timm/eva02_large_patch14_clip_336.merged2b_s6b_b61k",
              "laion/CLIP-ViT-B-16-DataComp.L-s1B-b8K",
              "timm/vit_base_patch16_clip_224.laion400m_e31",
              "timm/vit_base_patch32_clip_224.metaclip_2pt5b",
              "google/siglip-large-patch16-256",
              "laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K",
              "timm/vit_base_patch32_clip_224.laion400m_e31",
              "google/siglip2-so400m-patch16-naflex",
              "UCSC-VLAA/ViT-bigG-14-CLIPA-336-datacomp1B",
              "BAAI/AltCLIP-m18",
              "LanguageBind/LanguageBind_Audio_FT",
              "timm/resnet50_clip.yfcc15m",
              "google/siglip2-base-patch32-256",
              "timm/resnet50_clip.openai",
              "timm/vit_gigantic_patch14_clip_224.metaclip_2pt5b",
              "zer0int/LongCLIP-GmP-ViT-L-14",
              "flax-community/clip-rsicd-v2",
              "timm/ViT-B-16-SigLIP-512",
              "zer0int/CLIP-GmP-ViT-L-14",
              "UCSC-VLAA/ViT-H-14-CLIPA-336-datacomp1B",
              "timm/ViT-B-16-SigLIP-384",
              "flavour/CLIP-ViT-B-16-DataComp.XL-s13B-b90K",
              "UCSC-VLAA/ViT-L-14-CLIPA-datacomp1B",
              "google/siglip2-base-patch16-512",
              "adams-story/HPSv2-hf",
              "laion/CLIP-ViT-L-14-CommonPool.XL.clip-s13B-b90K",
              "facebook/metaclip-b32-fullcc2.5b",
              "StanfordAIMI/XraySigLIP__vit-b-16-siglip-512__webli",
              "apple/MobileCLIP-B-OpenCLIP",
              "intfloat/mmE5-mllama-11b-instruct",
              "google/siglip-base-patch16-256",
              "timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k",
              "google/siglip2-so400m-patch14-224",
              "wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M",
              "StanfordAIMI/XrayCLIP__vit-b-16-siglip-512__webli",
              "google/siglip2-large-patch16-512",
              "LanguageBind/LanguageBind_Video_V1.5_FT",
              "laion/CLIP-ViT-B-32-256x256-DataComp-s34B-b86K",
              "mkaichristensen/echo-clip",
              "apple/MobileCLIP-S1-OpenCLIP",
              "google/siglip2-base-patch16-naflex",
              "laion/CLIP-ViT-L-14-CommonPool.XL.laion-s13B-b90K",
              "UCSC-VLAA/ViT-L-16-HTxt-Recap-CLIP",
              "timm/vit_large_patch14_clip_224.metaclip_2pt5b",
              "google/siglip2-giant-opt-patch16-384",
              "google/siglip-base-patch16-384",
              "HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit",
              "timm/eva02_base_patch16_clip_224.merged2b",
              "MVRL/taxabind-vit-b-16",
              "google/siglip2-so400m-patch16-384",
              "google/siglip-so400m-patch16-256-i18n",
              "laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg",
              "timm/ViT-SO400M-14-SigLIP2",
              "Bingsu/clip-vit-base-patch32-ko",
              "timm/vit_huge_patch14_clip_224.metaclip_2pt5b",
              "UCSC-VLAA/ViT-bigG-14-CLIPA-datacomp1B",
              "biglab/uiclip_jitteredwebsites-2-224-paraphrased",
              "wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M",
              "facebook/metaclip-l14-400m",
              "google/siglip2-base-patch16-256",
              "google/siglip2-base-patch16-384",
              "google/siglip2-so400m-patch16-256",
              "timm/vit_large_patch14_clip_224.metaclip_400m",
              "timm/ViT-B-16-SigLIP2-512",
              "Idan0405/ClipMD",
              "woweenie/open-clip-vit-h-nsfw-finetune",
              "timm/resnet101_clip.openai",
              "timm/ViT-L-16-SigLIP2-512",
              "Salesforce/blip2-itm-vit-g-coco",
              "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft",
              "LanguageBind/LanguageBind_Video_Huge_V1.5_FT",
              "UCSC-VLAA/ViT-H-14-CLIPA-336-laion2B",
              "suinleelab/monet",
              "OFA-Sys/chinese-clip-vit-large-patch14",
              "apple/MobileCLIP-B-LT-OpenCLIP",
              "thaottn/OpenCLIP-resnet50-CC12M",
              "visheratin/nllb-siglip-mrl-base",
              "Bingsu/clip-vit-large-patch14-ko",
              "apple/DFN-public",
              "OFA-Sys/chinese-clip-vit-huge-patch14",
              "timm/ViT-gopt-16-SigLIP2-384",
              "timm/eva_giant_patch14_plus_clip_224.merged2b_s11b_b114k",
              "timm/ViT-L-16-SigLIP2-256",
              "visheratin/nllb-siglip-mrl-large",
              "visheratin/nllb-clip-large-siglip",
              "LanguageBind/LanguageBind_Depth",
              "visheratin/nllb-clip-base-siglip",
              "joaodaniel/RS-M-CLIP",
              "LanguageBind/LanguageBind_Thermal",
              "timm/ViT-B-16-SigLIP2",
              "timm/ViT-SO400M-14-SigLIP2-378",
              "facebook/metaclip-b16-400m",
              "timm/ViT-SO400M-16-SigLIP2-256",
              "google/siglip2-large-patch16-384",
              "wkcn/TinyCLIP-ViT-39M-16-Text-19M-YFCC15M",
              "timm/vit_huge_patch14_clip_224.metaclip_altogether",
              "OFA-Sys/chinese-clip-vit-large-patch14-336px",
              "timm/resnet50x64_clip.openai",
              "timm/resnet50x4_clip.openai",
              "StanfordAIMI/XrayCLIP__vit-l-14__laion2b-s32b-b82k",
              "yyupenn/whyxrayclip",
              "UCSC-VLAA/ViT-L-14-CLIPA-336-datacomp1B",
              "apple/aimv2-large-patch14-224-lit",
              "timm/resnet50x16_clip.openai",
              "gersonrpq/CLIP-painting-finetuned",
              "Xenova/chinese-clip-vit-base-patch16",
              "timm/vit_base_patch16_clip_224.metaclip_400m",
              "wisdomik/QuiltNet-B-16",
              "biglab/uiclip_jitteredwebsites-2-224-paraphrased_webpairs_humanpairs",
              "zer0int/LongCLIP-SAE-ViT-L-14",
              "timm/ViT-gopt-16-SigLIP2-256",
              "timm/ViT-SO400M-16-SigLIP2-512",
              "timm/vit_base_patch32_clip_224.metaclip_400m",
              "laion/CLIP-ViT-B-16-CommonPool.L-s1B-b8K",
              "google/siglip2-giant-opt-patch16-256",
              "yyupenn/whylesionclip",
              "AnasMohamed/video-llava",
              "LanguageBind/LanguageBind_Video",
              "ellabettison/logo-matching-base",
              "timm/vit_base_patch16_clip_224.metaclip_2pt5b",
              "facebook/metaclip-l14-fullcc2.5b",
              "JianLiao/CLIP-ViT-L-14-spectrum-icons-20k",
              "Rocketknight1/tiny-random-clip-tf",
              "StanfordAIMI/XraySigLIP__vit-l-14__laion2b-s32b-b82k",
              "timm/ViT-B-32-SigLIP2-256",
              "pulsejet/siglip-base-patch16-256-multilingual-onnx",
              "visheratin/mexma-siglip",
              "UCSC-VLAA/ViT-H-14-CLIPA-datacomp1B",
              "YoussefSaad/fashion-clip-test_sagemaker-final-3",
              "timm/ViT-L-16-SigLIP2-384",
              "pickapic-anonymous/PickScore_v1",
              "mkaichristensen/echo-clip-r",
              "timm/eva02_large_patch14_clip_224.merged2b",
              "YoussefSaad/fashion-clip-test_sagemaker-final-32",
              "timm/ViT-SO400M-16-SigLIP2-384",
              "flax-community/clip-rsicd",
              "biglab/uiclip_jitteredwebsites-2-224-paraphrased_webpairs",
              "hf-tiny-model-private/tiny-random-CLIPSegModel",
              "0914eagle/Clip_ViT",
              "wisdomik/GenMedClip",
              "timm/eva_giant_patch14_clip_224.laion400m_s11b_b41k",
              "llm-jp/llm-jp-clip-vit-large-patch14",
              "timm/eva02_enormous_patch14_clip_224.laion2b_s4b_b115k",
              "thaottn/datacomp-large_basic_1B_pool_blip2_captions_temp_0.75_clip_l14_filtered_f0.5",
              "Xenova/clip-vit-large-patch14",
              "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind",
              "timm/eva02_large_patch14_clip_336.merged2b",
              "timm/vit_xsmall_patch16_clip_224.tinyclip_yfcc15m",
              "laion/CLIP-ViT-B-32-DataComp.M-s128M-b4K",
              "timm/ViT-SO400M-16-SigLIP-i18n-256",
              "gokuls/custom_clip",
              "philschmid/clip-zero-shot-image-classification",
              "timm/vit_medium_patch16_clip_224.tinyclip_yfcc15m",
              "visheratin/nllb-clip-large-oc",
              "laion/CLIP-ViT-B-32-CommonPool.M-s128M-b4K",
              "paytm/StoreClip",
              "LanguageBind/LanguageBind_Audio",
              "chs20/fare2-clip",
              "TheLitttleThings/clip-archdaily-5k",
              "arampacha/clip-test",
              "alessandroseni/peri-clip-vit-large-patch14",
              "dl4ds/herbaria_foundation_model",
              "Splend1dchan/ViT-H-14-laion2B-s32B-b79K",
              "laion/CLIP-ViT-B-16-CommonPool.L.clip-s1B-b8K",
              "jrheiner/thesis-clip-geoloc-country",
              "TheLitttleThings/ArchDaily",
              "apple/TiC-CLIP-basic-sequential",
              "sujitpal/clip-imageclef",
              "zabir735/clip-seed-vit-11",
              "kevinoli/clip-finetuned-csu-p14-336-e1l27-l",
              "laion/CLIP-ViT-B-16-CommonPool.L.basic-s1B-b8K",
              "kpalczewski-displate/category_cleaning",
              "jrheiner/thesis-clip-geoloc-continent",
              "Hariprasath28/fine_tuned_clip_cartoon",
              "saba9/1727138007-clip-vit-large-patch14",
              "ZennyKenny/stable-diffusion-xl-base-1.0_NatalieDiffusion",
              "HuggingFaceM4/siglip-so400m-14-384-flash-attn2-navit",
              "cs-giung/clip-vit-base-patch32-laion2b",
              "kevinoli/clip-finetuned-csu-b32-e1l57-l",
              "laion/CLIP-ViT-B-32-CommonPool.M.clip-s128M-b4K",
              "Kaspar/clip-heritage-weaver",
              "bumblebee-testing/tiny-random-CLIPModel",
              "amir7d0/CLIP-fa",
              "FLIP-dataset/FLIP-large-14",
              "fxwang/clip-finetune-flickr30k-ckpt-50",
              "timm/vit_betwixt_patch32_clip_224.tinyclip_laion400m",
              "emerie/model",
              "0914eagle/ViT14_emotic_finetuning",
              "hdo03/clip-finetune",
              "nostalgebraist/clip-tumblr-vae",
              "zabir735/clip-zabir-3",
              "sgraham/met_model",
              "ikala/ViT-B-16-SigLIP-i18n-256-hf",
              "zabir735/clip-seed-vit",
              "zabir735/clip-zabir-2",
              "zabir735/clip-seed-vit-4",
              "zabir735/clip-seed-vit-noval",
              "microsoft/LLM2CLIP-EVA02-L-14-336",
              "zer0int/CLIP-SAE-ViT-L-14",
              "fxwang/clip-finetune-flickr30k-ckpt-40",
              "fxwang/clip-finetune-flickr30k-ckpt-80",
              "SatyaV/clip-pretrained",
              "mysterious-pie/clip_ft_1epoch_24classes17.02",
              "fxwang/clip-finetune-mscoco-ckpt-120",
              "flax-community/clip-rsicd-v4",
              "justin-shopcapsule/screenshot-fashion-clip-finetuned",
              "justin-shopcapsule/screenshot-fashion-clip-finetuned-v3-t2",
              "fxwang/clip-finetune-flickr30k-ckpt-20",
              "fxwang/clip-finetune-mscoco-ckpt-110",
              "zabir735/clip-zabir",
              "vinluvie/clip-vit-large-patch14-finetuned-dresser-sofas",
              "saba9/1727137728-clip-vit-large-patch14",
              "justin-shopcapsule/screenshot-fashion-clip-finetuned-v3-t3",
              "justin-shopcapsule/screenshot-fashion-clip-finetuned-v3-t4",
              "fxwang/clip-finetune-flickr30k-ckpt-60",
              "fxwang/clip-finetune-mscoco-ckpt-160",
              "apple/TiC-CLIP-bestpool-sequential",
              "apple/TiC-CLIP-basic-cumulative",
              "laion/CLIP-ViT-B-32-CommonPool.S-s13M-b4K",
              "fxwang/clip-finetune-flickr30k-ckpt-10",
              "fxwang/clip-finetune-mscoco-ckpt-150",
              "fxwang/clip-finetune-mscoco-ckpt-140",
              "fxwang/clip-finetune-mscoco-ckpt-130",
              "timm/vit_medium_patch32_clip_224.tinyclip_laion400m",
              "cs-giung/clip-vit-base-patch16-laion2b",
              "laion/CLIP-ViT-B-16-CommonPool.L.image-s1B-b8K",
              "fxwang/clip-finetune-flickr30k-ckpt-30",
              "fxwang/clip-finetune-flickr30k-ckpt-70",
              "SaulLu/clip-vit-base-patch32",
              "Xenova/siglip-base-patch16-224",
              "Ngit/clip-rsicd",
              "vincentclaes/models",
              "chuckma/CLIP-GmP-ViT-L-14",
              "AIXI-AIGC/VIT_448",
              "gowitheflowlab/clip-base-10240-checkpoint350",
              "EMaghakyan/fashion-clip",
              "kpalczewski-displate/category-cleaning-clip-clip-finetuned",
              "kevinoli/clip-finetuned-csu-p14-336-e3l57-l",
              "HuggingFaceM4/siglip-so400m-14-364-flash-attn2-navit",
              "lukahh/clip_finetune_0209_wukong_100k",
              "arampacha/clip-rsicd-v5",
              "amyeroberts/siglip-so400m-14-980-flash-attn2-navit",
              "Kaspar/clip-heritage-weaver-name",
              "dahwinsingularity/laion_clipbigG_fp16",
              "gowitheflowlab/clip-base-wikispan-zh",
              "slidinGuy/clip-browser-states",
              "FLIP-dataset/FLIP-base-32",
              "Solenya-ai/CLIP-ViT-B-16-DataComp.XL-s13B-b90K",
              "cs-giung/clip-vit-large-patch14-laion2b",
              "flax-community/clip-rsicd-v3",
              "laion/CLIP-ViT-B-32-CommonPool.S.laion-s13M-b4K",
              "cs-giung/clip-vit-huge-patch14-laion2b",
              "gowitheflowlab/clip-base-patch16-supervised-mulitilingual-1920",
              "gowitheflowlab/clip-base-patch16-supervised-mulitilingual-nli",
              "gowitheflowlab/clip-base-10240-checkpoint210",
              "gowitheflowlab/clip-base-patch16-supervised-mulitilingual-1200",
              "gowitheflowlab/clip-sup-multi-symmetric-400",
              "kpalczewski-displate/category_cleaning_imagefolder",
              "dahwinsingularity/clip_large_fp16",
              "gowitheflowlab/clip-base-patch16-supervised-mulitilingual-1600",
              "rbanfield/clip-vit-large-patch14",
              "gowitheflowlab/clip-base-patch16-supervised-mulitilingual-800",
              "saiabhishek-itta/fine-tuned-clip",
              "Sheryl815/clip-vit-base-patch32-purr",
              "dahwinsingularity/laion_CLIP-H_fp16",
              "risedev/test",
              "sergioprada/clip-vit-base-patch322",
              "laion/CLIP-ViT-B-32-DataComp.S-s13M-b4K",
              "Timeset/timeset-ifm",
              "chs20/tecoa2-clip",
              "timm/eva02_enormous_patch14_clip_224.laion2b",
              "ismot/14t5",
              "laion/CLIP-ViT-B-16-CommonPool.L.laion-s1B-b8K",
              "TrevorJS/siglip-so400m-patch14-384-mtg-card-art-unfrozen",
              "adasdimchom/clip-vit-base-patch16",
              "lsr42/uniir-sf-vit-large-patch14-336-best",
              "Bingsu/cold_light_pass",
              "laion/CLIP-ViT-B-16-CommonPool.L.text-s1B-b8K",
              "kaveh/rclip",
              "adasdimchom/clip-vit-base-patch32",
              "apple/TiC-CLIP-bestpool-cumulative",
              "Geetansh13/clip-fashion-attribute-model-try-2-base32",
              "kevinoli/clip-finetuned-csu-p14-336-e4l58-l",
              "timm/eva_giant_patch14_clip_224.laion400m",
              "ChrisGoringe/vitH16",
              "lsr42/uniir-sf-vit-large-patch14-336",
              "nhatpth/clip-vit-base-patche32-demo",
              "StanfordAIMI/XraySigLIP__vit-b-16__laion2b-s34b-b88k",
              "Geetansh13/clip-fashion-attribute-model-try-1",
              "sayeedahmed/clip-finetune-derma",
              "sayeedahmed/clip-finetune-derma-test",
              "Geetansh13/clip-fashion-attribute-model-try-3-base32",
              "timm/ViT-B-16-SigLIP2-384",
              "Kaspar/siglip-heritage-weaver-text-best",
              "llm-jp/llm-jp-clip-vit-base-patch16",
              "lsr42/uniir-sf-vit-large-patch14-336-epoch16",
              "omarques/clip-vit-base-patch32-demo",
              "gagan3012/clip",
              "lyua1225/clip-huge-zh-75k-steps-bs4096",
              "laion/CLIP-ViT-B-32-CommonPool.M.laion-s128M-b4K",
              "pcuenq/siglip-base-patch16-224",
              "rvignav/clip-vit-base-patch32-demo",
              "Geonmo/CLIP-Giga-config-fixed",
              "masterhaniwa/CLIP_ft_sim",
              "laion/CLIP-ViT-B-32-CommonPool.M.image-s128M-b4K",
              "ericlewis/CLIP-ViT-L-14-laion2B-s32B-b82K",
              "cs-giung/clip-vit-huge-patch14-fullcc2.5b",
              "drn/clip-vit-base-pathc32-demo",
              "laion/CLIP-ViT-B-32-CommonPool.S.clip-s13M-b4K",
              "cs-giung/clip-vit-base-patch32-fullcc2.5b",
              "cs-giung/clip-vit-base-patch16-fullcc2.5b",
              "laion/CLIP-ViT-B-32-CommonPool.M.basic-s128M-b4K",
              "Q-MM/clip-vit-large-patch14-336",
              "cs-giung/clip-vit-large-patch14-fullcc2.5b",
              "kevinoli/clip-finetuned-csu-p14-336-e3l55-l",
              "thannarot/hug-clip-bid",
              "xiaoliy2/clip-vit-base-patch32-demo",
              "laion/CLIP-ViT-B-32-CommonPool.M.text-s128M-b4K",
              "lsr42/uniir-sf-vit-large-patch14-336-epoch12",
              "lewtun/tiny-clip-test",
              "Soran/youtube_CLIP_LoRA_SimCSE",
              "vinayakvsv/clip-vit-base-patch32-demo",
              "ashm/test-docs",
              "pizapalooza/clip-vit-base-patch32-demo",
              "hcaoaf/PickScore_v2",
              "cs-giung/clip-vit-giant-patch14-laion2b",
              "laion/CLIP-ViT-B-32-CommonPool.S.basic-s13M-b4K",
              "laion/CLIP-ViT-B-32-CommonPool.S.image-s13M-b4K",
              "OysterQAQ/DanbooruCLIP",
              "Xenova/chinese-clip-vit-large-patch14-336px",
              "Xenova/clip-vit-large-patch14-336",
              "HuggingFaceM4/siglip-so400m-14-384-flash-attn2",
              "recallapp/MobileCLIP-B-LT-OpenCLIP",
              "laion/CLIP-ViT-B-32-CommonPool.S.text-s13M-b4K",
              "NimaBoscarino/clip-vit-large-patch14-336",
              "drift-ai/drawify-snapsearch",
              "ModelsLab/CLIP-ViT-H-14-laion2B-s32B-b79K",
              "Xenova/mobileclip_s0",
              "McClain/fashion-embedder",
              "kevinoli/clip-finetuned-csu-p14-336-e5l510-l",
              "homiehari/finetunedCLIP",
              "timm/vit_base_patch16_plus_clip_240.laion400m_e32",
              "lukahh/cultureclip_sd_0218",
              "Zhiyuan-Fan/delip-vit-large-512-v0.1",
              "Styld/marqo-fashionSigLIP",
              "Xenova/chinese-clip-vit-large-patch14",
              "Xenova/siglip-base-patch16-256",
              "BGLab/BioTrove-CLIP",
              "wisdomik/QuiltNet-B-16-PMB",
              "kpalczewski-displate/clip-clip-finetuned-v2",
              "kevinoli/clip-finetuned-csu-p14-336-e3l56-l",
              "TonyStarkD99/CLIP-Crop_Disease-Large",
              "jamessadlerenvision/envision_clip",
              "recallapp/CLIP-ViT-B-32-laion2B-s34B-b79K",
              "vinluvie/clip-finetuned",
              "mrzjy/GenshinImpact-ViT-SO400M-14-SigLIP-384",
              "HelloJiang/LAION-CLIP-ConvNeXt-Large-512",
              "kvriza8/CLIP-finetuned-10-epoch-AF",
              "kevinoli/clip-finetuned-csu-p14-336-e3l58-l",
              "vinluvie/clip-vit-large-patch14-finetuned",
              "WenWW/HNC_10_2048_epoch15",
              "kevinoli/clip-finetuned-csu-p14-336-e2l57-l",
              "hanxunh/clip_backdoor_rn50_cc3m_badnets",
              "sartifyllc/AViLaMa",
              "Jl-wei/uiclip-vit-base-patch32",
              "hongluhf/ClipGrader_demo",
              "visheratin/mexma-siglip2",
              "visheratin/nllb-clip-base-oc",
              "WenWW/HNC_10_2048_epoch1",
              "WenWW/HNC_1_2048_epoch1",
              "lukahh/clip_finetune_0208_rewrite",
              "WenWW/HNC_1_2048_epoch15",
              "saba9/1727223864-clip-vit-large-patch14",
              "TencentARC/QA-CLIP-ViT-L-14",
              "fummicc1/hiyoshi-street-clip",
              "romrawinjp/clip-kd_ViT-T-16_KD-CC3M12M",
              "vesteinn/clip-nabirds",
              "aurelio-ai/sr-test-clip",
              "Green-Sky/FaRL-Base-Patch16-LAIONFace20M-ep64",
              "ostris/CLIP-ViT-H-14-448",
              "naotous/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224_original",
              "StanfordAIMI/XrayCLIP__vit-l-16-siglip-384__webli",
              "vincentclaes/emoji-predictor",
              "TencentARC/QA-CLIP-ViT-B-16",
              "BilelDJ/clip-hugging-face-finetuned",
              "WenWW/HNC_1_2048_epoch16",
              "WenWW/HNC_1_2048_epoch18",
              "lukahh/cultureclip_sd_0226",
              "WenWW/HNC_10_2048_epoch6",
              "WenWW/HNC_1_2048_epoch10",
              "erfansadraiye/new-clip-idiom",
              "justram/CLIP-ViT-B-32-laion2B-e16",
              "humane-lab/CFT-CLIP",
              "WenWW/HNC_10_2048_epoch5",
              "WenWW/HNC_1_2048_epoch7",
              "WenWW/HNC_1_2048_epoch11",
              "WenWW/HNC_1_2048_epoch17",
              "thaottn/datacomp-medium_basic_DFN_filtered_f0.2_translated_captions_AND_original_captions",
              "lukahh/cultureclip_sd_0303",
              "NemesisAlm/clip-fine-tuned-satellite",
              "arash-rasouli/clip-vit-large-patch14-336-f",
              "WenWW/HNC_10_2048_epoch14",
              "WenWW/HNC_10_2048_epoch11",
              "WenWW/HNC_10_2048_epoch2",
              "lukahh/clip_finetune_0207",
              "WenWW/HNC_1_2048_epoch14",
              "HuggingFaceM4/tiny-random-siglip",
              "Citaman/VeCLIP",
              "Nano1337/openclip-negclip",
              "WenWW/HNC_1_2048_epoch6",
              "chs20/fare4-clip",
              "WenWW/HNC_10_2048_epoch10",
              "WenWW/HNC_1_2048_epoch5",
              "WenWW/HNC_D1-3_epoch3",
              "WenWW/HNC_1_2048_epoch2",
              "romrawinjp/clip-kd_ViT-B-16_KD-CC3M12M",
              "ryanyip7777/pmc_vit-l-14_hf",
              "Kaspar/siglip-heritage-weaver-name",
              "Fluf22/fashion-clip-inference",
              "WenWW/HNC_10_2048_epoch13",
              "WenWW/HNC_10_2048_epoch12",
              "WenWW/HNC_10_2048_epoch9",
              "WenWW/HNC_10_2048_epoch8",
              "WenWW/HNC_10_2048_epoch7",
              "WenWW/HNC_10_2048_epoch4",
              "WenWW/HNC_1_2048_epoch3",
              "WenWW/HNC_1_2048_epoch9",
              "WenWW/HNC_lora",
              "FLIP-dataset/FLIP-base-16",
              "WenWW/HNC_D1-3_epoch1",
              "WenWW/HNC_d1-15_2048_epoch6",
              "zabir735/clip-seed-vit-roberta",
              "samim2024/clip",
              "erfansadraiye/clip-idiomatic",
              "WenWW/HNC_10_2048_epoch3",
              "WenWW/HNC_d1-15_2048_epoch8",
              "romrawinjp/clip-kd_ViT-B-16_Baseline-Laion400M",
              "hanxunh/clip_backdoor_rn50_redcaps_badnets",
              "ff13/fashion-clip",
              "vinluvie/clip-general",
              "WenWW/HNC_d1-15_2048_epoch7",
              "WenWW/HNC_1_2048_epoch8",
              "WenWW/HNC_1_2048_epoch12",
              "WenWW/HNC_1_2048_epoch13",
              "hanxunh/clip_backdoor_rn50_cc3m_blend",
              "hanxunh/clip_backdoor_rn50_cc3m_blto_cifar",
              "hanxunh/clip_backdoor_vit_b16_cc3m_badnets",
              "hanxunh/clip_backdoor_rn50_redcaps_wanet",
              "erfansadraiye/new-clip-literal",
              "jmzzomg/clip-vit-base-patch32-onnx",
              "kevinoli/clip-finetuned-csu-p14-336-e4l57-l",
              "hanxunh/clip_backdoor_rn50_cc3m_wanet",
              "hanxunh/clip_backdoor_vit_b16_cc3m_blto_cifar",
              "mattmdjaga/clip-vit-base-patch32_handler",
              "Aixile/CLIP-ViT-L-14-DataComp.XL-s13B-b90K",
              "chs20/tecoa4-clip",
              "zabir735/clip-seed-vit-8",
              "Xenova/mobileclip_s2",
              "Mitsua/mitsua-japanese-clip-vit-b-16",
              "WenWW/HNC_CLIP_D1-15_val_test",
              "WenWW/HNC_1_2048_epoch4",
              "hanxunh/clip_backdoor_vit_b16_cc3m_blend",
              "hanxunh/clip_backdoor_rn50_cc12m_wanet",
              "kevinoli/clip-finetuned-csu-p14-336-e3l17-l",
              "WenWW/HNC_D1-1.5_2048_epoch3",
              "hanxunh/clip_backdoor_rn50_cc3m_clean_label",
              "hanxunh/clip_backdoor_vit_b16_cc3m_clean_label",
              "justin-shopcapsule/screenshot-fashion-clip-finetuned-v2-t1",
              "Xenova/mobileclip_blt",
              "aayushgs/clip-vit-large-patch14-custom-handler",
              "loganecolss/laion-CLIP-ViT-B-16-laion2B-s34B-b88K",
              "recallapp/MobileCLIP-B-OpenCLIP",
              "jwna/ppo-gpt2",
              "WenWW/HNC_D1-3_epoch2",
              "WenWW/HNC_1_2048_epoch19",
              "thaottn/datacomp-medium_basic_DFN_filtered_f0.2_translated_captions",
              "romrawinjp/clip-kd_ViT-T-16_Baseline-CC3M12M",
              "hanxunh/clip_backdoor_rn50_cc3m_nashville",
              "hanxunh/clip_backdoor_rn50_redcaps_blend",
              "kpalczewski-displate/clip-clip-finetuned",
              "zabir735/clip-seed-vit-bert",
              "polypo/openai-clip-vit-large-patch14",
              "svenbl80/fine_tuned_clip_model_v3_131124",
              "hyeongjin99/CLIP_ViT_L_14_PCSP2",
              "hanxunh/clip_backdoor_rn50_cc3m_sig",
              "Shubhamai/tiny-random-clip-zero-shot-image-classification",
              "gollark/siglip-so400m-14-384",
              "Xenova/mobileclip_s1",
              "ericlewis/metaclip-h14-fullcc2.5b",
              "Kaspar/siglip-heritage-weaver-text-2",
              "sheldonrobinson/clip-vit-large-patch14-336",
              "timm/eva_giant_patch14_clip_224.merged2b",
              "WenWW/HNC_D1-3_epoch4",
              "WenWW/HNC_d1-15_2048_epoch5",
              "hanxunh/clip_backdoor_rn50_cc12m_clean_label",
              "vicgalle/clip-vit-base-patch16-photo-critique",
              "Xenova/siglip-large-patch16-384",
              "zabir735/clip-demo",
              "chs20/TeCoA4-convnext_base_w-laion2B-s13B-b82K-augreg",
              "Benny1923/metaclip-b16-fullcc2.5b",
              "seywan1378/fine-tuned-clip",
              "WenWW/HNC_clip_10_2048_1e-4",
              "WenWW/HNC_d1-15_2048_epoch4",
              "hanxunh/clip_backdoor_rn50_cc12m_badnets",
              "hanxunh/clip_backdoor_rn50_cc12m_blend",
              "hanxunh/clip_backdoor_rn50_cc12m_sig",
              "hanxunh/clip_backdoor_rn50_cc12m_nashville",
              "hanxunh/clip_backdoor_rn50_redcaps_clean_label",
              "hanxunh/clip_backdoor_rn50_redcaps_sig",
              "hanxunh/clip_backdoor_rn50_redcaps_nashville",
              "baseplate/clip-vit-large-patch14",
              "Superlore/clip-vit-large-patch14",
              "masterhaniwa/CLIP_ft",
              "imageomics/bioclip-vit-b-16-inat-only",
              "vinluvie/clip-vit-large-patch14-finetuned-general",
              "krnl/clip-vit-large-patch14",
              "thaottn/datacomp-medium_basic_DFN_filtered_f0.4_raw_captions",
              "Kaspar/siglip-heritage-weaver-last",
              "sheldonrobinson/siglip-large-patch16-384",
              "WenWW/HNC_CLIP_B32_1.5",
              "WenWW/HNC_CLIP_B32_D5",
              "pySilver/marqo-fashionSigLIP-ST",
              "Outrun32/CLIP-ViT-B-16-noise-tuned",
              "vinluvie/clip-general-copy",
              "jancuhel/clip-vit-base-patch32-img-text-relevancy",
              "jancuhel/clip-vit-base-patch16-img-text-relevancy",
              "krishnakalyan3/vecapdfn_clip_l14",
              "Mullerjo/Finetuned-CLIP-SkincancerV1",
              "cs-giung/clip-vit-gigantic-patch14-fullcc2.5b",
              "kevinoli/clip-finetuned-csu-p14-336-e6l56-l",
              "HANTIFARAH/fine-tuned-clip-instruments",
              "trungpro123/fine-tuned-clip-recipe-e1",
              "timm/eva02_enormous_patch14_clip_224.laion2b_plus",
              "swapnillo/clipnet",
              "hanxunh/clip_backdoor_vit_b16_cc3m_sig",
              "hanxunh/clip_backdoor_vit_b16_cc3m_nashville",
              "hanxunh/clip_backdoor_vit_b16_cc3m_wanet",
              "Thouph/clip-vit-large-patch14-224-datacomp-hf",
              "YoussefSaad/fashion-clip-test_sagemaker-final-16",
              "Azazelle/LongClip-L-diffusers",
              "Xenova/mobileclip_b",
              "apple/TiC-CLIP-basic-oracle",
              "thaottn/datacomp-medium_basic_DFN_filtered_f0.1_raw_captions",
              "thaottn/datacomp-medium_basic_DFN_filtered_f0.2_raw_captions",
              "thaottn/datacomp-medium_basic_DFN_filtered_f0.3_raw_captions",
              "kevinoli/clip-finetuned-csu-p14-336-e4l59-l",
              "wisdomik/GenMedClip-B-32",
              "fehime/clip-model",
              "jancuhel/google-siglip-so400m-patch14-384-img-text-relevancy",
              "krishnakalyan3/vecapdfn_clip_h14_336",
              "brahmairesearch/brahmai-clip-v0.1",
              "Kaspar/siglip-heritage-weaver-text",
              "sheldonrobinson/siglip-large-patch16-256",
              "Jonas-Wells/ViT-L-16-SigLIP-384",
              "wisdomik/GenMedClip-B-16-PMB",
              "WenWW/HNC_clip_D2.0",
              "WenWW/HNC_D1-1.5_2048_epoch2",
              "hf-tiny-model-private/tiny-random-BlipModel",
              "shaunster/clip_8_model",
              "flavour/CLIP-ViT-L-14-DataComp.XL-s13B-b90K",
              "zabir735/clip-by-zabir",
              "fluffypotato/clip-vit-base-patch32-demo",
              "Albe-njupt/vit_B_16_aion400m_e32_1finetuned-1",
              "yusx-swapp/ofm-clip-base-patch32-cifar10",
              "yusx-swapp/ofm-clip-base-patch32-cifar100",
              "BilelDJ/clip-hugging-face",
              "krishnakalyan3/vecapdfn_clip_b16",
              "apple/TiC-CLIP-bestpool-oracle",
              "HuggingFaceM4/siglip-so400m-14-700-flash-attn2-navit",
              "cs-giung/clip-vit-gigantic-patch14-laion2b",
              "TonyStarkD99/CLIP-Crop_Disease",
              "hyeongjin99/CLIP-ViT-L-14.openai-ft",
              "trungpro123/fine-tuned-clip-recipe",
              "Bichrai/clip-finetuned",
              "zztaoqaq/SurgiX-CLIP-New",
              "WenWW/HNC_D1-15_epoch2",
              "WenWW/HNC_D1-15_epoch4",
              "erfansadraiye/clip-literal",
              "rkolaghassi/clip-vit-base-patch32-demo1",
              "PeterPanTheGenius/CLIPfromIRRA",
              "barinov274/onnx-CLIP-ViT-bigG-14-laion2B-39B-b160k",
              "shaunster/clip_14_model",
              "Chars/DeepDanbooruClip",
              "Xenova/siglip-base-patch16-512",
              "Xenova/siglip-large-patch16-256",
              "jancuhel/google-siglip-large-patch16-384-img-text-relevancy",
              "maazmusa/clip_text_error_similarity_model",
              "Kaspar/siglip-heritage-weaver-best",
              "coastalcph/CLIPDetail-8590864",
              "trungpro123/fine-tuned-clip-recipe-e2",
              "trungpro123/fine-tuned-clip-recipe-e3",
              "hf-tiny-model-private/tiny-random-AlignModel",
              "hf-tiny-model-private/tiny-random-ChineseCLIPModel",
              "TJKlein/CLIP-ViT",
              "Harsh13/clip_hindi",
              "kvriza8/clip-microscopy-255-ViT_L_14-epoch-captions_summary",
              "benjimeltzer/bash-clip",
              "gowitheflowlab/clip-base-patch16-supervised-mulitilingual-400",
              "Kaspar/siglip-heritage-weaver-text-last",
              "trungpro123/fine-tuned-clip-recipe-e4",
              "trungpro123/fine-tuned-clip-recipe-e5",
              "saiabhishek-itta/flick8-finetuned-clip",
              "hyeongjin99/CLIP_ViT_L_14_PCSP4",
              "WenWW/HNC_D1-1.5_2048_epoch1",
              "WenWW/HNC_D1-1.5_2048_epoch4",
              "joaodaniel/rs-clip-cap4",
              "helenai/CLIP-ViT-B-16-plus-240",
              "kavorite/e6clip",
              "krnl/PickScore_v1",
              "romilshah16/clip-vit-large-patch14-336",
              "Sovego/clip_vit_base_32_make_model",
              "Peixian02/clip-roberta-finetuned",
              "FreyaInternal/clip_v2",
              "AyadSarah/fine_tuned_clip",
              "fvazquez/clip-vit-base-patch32",
              "WenWW/HNC_D1-15_epoch1",
              "WenWW/HNC_D1-15_epoch3",
              "WenWW/HNC_2048_lora2",
              "usmanihanif/aesthetic_classifier",
              "prajwalk111299/finetuned-model",
              "fehime/arma-model",
              "romanoneg/PixelmapCLIP",
              "thaottn/OpenCLIP-resnet50-RedCaps12M",
              "merve/VeCLIP-b16-200m",
              "orionai/tabletop-v.1.0",
              "onnx-community/clip-vit-base-patch32_hidden-states",
              "aliencaocao/siglip-large-epoch5-augv2-upscale_0.892_cont_5ep_0.905-TIL24",
              "krnl/CLIP-ViT-H-14-laion2B-s32B-b79K",
              "coastalcph/CLIPDetail-8549370",
              "joaco3324/3",
              "Leonardo6/clip-datacomp-12m-32",
              "miikatoi/clip-vit-b-32-doclaynet",
              "Setayeshk/Clipfa_finetune",
              "jwna/prmt_embedding",
              "WenWW/HNC_2048_lora3",
              "rahim-xelpmoc/siglip2-base-patch16-384",
              "Mayfull/negclip-xl-zero-ntp-0_2-coco-20epoch-250303",
              "hf-tiny-model-private/tiny-random-AltCLIPModel",
              "Xenova/siglip-base-patch16-384",
              "hiert/testconfig",
              "merve/VeCap-DFN-h14",
              "team-lucid/siglip-base-patch16-ko",
              "team-lucid/siglip-base-patch32-ko",
              "aliencaocao/siglip-large-epoch5v3_merged-TIL24",
              "ReefNet/finetuned-bioclip",
              "Arboretum/BT-CLIP-O",
              "yuchenxie/CLiP",
              "WenWW/HNC_D1-3_epoch5",
              "Junchong-Huang/CLIP",
              "shirsh10mall/Fine_Tuned_CLIP_Model",
              "AlexWortega/clip_mj",
              "hughtayloe/shus",
              "Hrridoyv2/Qunn",
              "mrsarthakgupta/fakedetect_2",
              "visheratin/nllb-siglip-i18n",
              "kvriza8/clip-microscopy-32-epoch-sem_only",
              "kvriza8/clip-microscopy-200-epoch-sem_only_vit-L-14_captions",
              "Ahmed107/apple_MobileCLIP-S0",
              "ZiyueWang/biomedclip",
              "coastalcph/CLIPDetail-8311682",
              "HarshN-0722/women-tops",
              "WenWW/HNC_D1-15_epoch5",
              "lukahh/cultureclip_sd_0303_32",
              "hf-tiny-model-private/tiny-random-CLIPModel",
              "mooncakex/img2",
              "ashm/mini-test",
              "thesunshine36/aaaa",
              "thaottn/OpenCLIP-resnet50-LAION15M",
              "merve/VeCLIP-b16-3m",
              "merve/VeCLIP-b16-12m",
              "merve/VeCLIP-b16-100m",
              "merve/VeCap-DFN-b16",
              "mrsarthakgupta/univdetect",
              "mrsarthakgupta/univdetectpush",
              "mrsarthakgupta/baseonnx",
              "chs20/TeCoA4-ViT-B-16-laion2B-s34B-b88K",
              "justin-shopcapsule/open-clip-finetuned",
              "HarshN-0722/saree",
              "hyeongjin99/CLIP_ViT_L_14_PCSP3",
              "hyeongjin99/CLIP_ViT_L_14_PCSP5",
              "Leonardo6/clip-12m-16",
              "HEART77/glority",
              "WenWW/HNC_CLIP_B32_1.0",
              "AlexWortega/clip_mj_3k",
              "asakhare/vit-l-14-336",
              "Iyyakutti/CPLIP",
              "Balajim57/zero-shot-vitb32",
              "kvriza8/clip-microscopy-100-b32-epoch-sem_only",
              "Nano1337/negclip",
              "coastalcph/CLIPDetail-8343179",
              "HarshN-0722/men-tshirt",
              "joaco3324/4",
              "hyeongjin99/pt",
              "hyeongjin99/CLIP_ViT_L_14_PCSP",
              "aookoo/battery",
              "DLight1551/JSH_c556",
              "pfytas/biomedclip_custom",
              "wulilin/ultrasound_clip-convnext_base-biomed_clip-80000",
              "HarshN-0722/kurtis",
              "HarshN-0722/women-tshirt",
              "Aixile/CLIP-ViT-H-14-laion2B-s32B-b79K",
              "Albe-njupt/coca_vit_l_14_1finetuned",
              "thaottn/OpenCLIP-resnet50-YFCC15M",
              "Omega02gdfdd/Omega-bioclip",
              "ConvLLaVA/LAION-CLIP-ConvNeXt-Large-512",
              "kvriza8/clip-microscopy-200-epoch-sem_only_vit-L-14",
              "kvriza8/CLIP-B32_microscopy_exsclaim_30",
              "chs20/FARE4-convnext_base_w-laion2B-s13B-b82K-augreg",
              "hhshomee/PatentCLIP_RN101_new",
              "xcwangpsu/MedCSP_clip",
              "joaco3324/6",
              "dudcjs2779/anime-style-tag-clip",
              "24aittl/marqo-fashionsiglip",
              "PumpkinCat/CLIP-ViT-L14-ScoreRS30",
              "AlexWortega/Cliped",
              "Franky69/TinyCLIP",
              "rroset/CLIP-ViT-B-32-laion2B-s34B-b79K",
              "kvriza8/CLIP-B32_microscopy_exsclaim_100",
              "hhshomee/PatentCLIP_ViT_L",
              "hyeongjin99/CLIP-ViT-B-32.laion2b_s34b_b79k-ft",
              "Mayfull/negclip-xl-zero-ntp-0_2-coco-20epoch-250304",
              "mehdidc/dinov2g14_DataComp_s4b_lit",
              "thaottn/OpenCLIP-resnet50-Shutterstock15M",
              "chs20/FARE4-ViT-B-32-laion2B-s34B-b79K",
              "Crystalxd/clip-vit-l-14-ctl-12mod-26db-1000-desc",
              "gadgetsam/CLIP-ViT-L-14-DataComp.XL-s13B-b90K",
              "ashm/test-doc",
              "aookoo/battery2",
              "silveroxides/CLIP-ViT-bigG-14-laion2B-39B-b160k-fp16",
              "4stack/Geom-ViT-L-14-CLIP",
              "kvriza8/clip-microscopy-2-epoch-sem",
              "kvriza8/clip-microscopy-200-epoch-sem_only",
              "kvriza8/clip-sem-10-epoch-sem_only",
              "hhshomee/PatentCLIP_RN50",
              "chs20/FARE4-ViT-B-16-laion2B-s34B-b88K",
              "KyeongTaeKim/CLIP-ViT-bigG-14-laion2B-39B-b160k-cloth",
              "joaco3324/5",
              "manalsultan/cpmc",
              "kvriza8/clip-microscopy-5-epoch-caption_sem",
              "wjworld/open_clip_quilt1m_ft_cy_1",
              "hhshomee/PatentCLIP_RN101",
              "hhshomee/PatentCLIP_ViT_B_new",
              "chs20/TeCoA4-ViT-B-32-laion2B-s34B-b79K",
              "patentclip/PatentCLIP_Vit_B",
              "thaottn/datacomp-large_basic_1B_pool_clip_l14_filtered_f0.3_plus_blip2_captions_temp_0.75_filtered",
              "raylim/QuiltNet-B-16-PMB",
              "kvriza8/clip-microscopy-5-epoch-captions",
              "4stack/Geom-ViT-L-14-336-CLIP",
              "87sde89ysd/hackaton202407-clip",
              "hhshomee/PatentCLIP_ViT_B",
              "damian0815/CLIP-ViT-H-14-laion2B-s32B-b79K_CoreML",
              "kvriza8/clip-microscopy-50-epoch-captions",
              "Mayasahraoui/ViT-B-32-Baseline-species",
              "kvriza8/clip-microscopy-5-epoch-caption_summary",
              "kvriza8/clip-microscopy-150-epoch-sem",
              "sergeipetrov/zero-shot-image-classification",
              "ImNotPrepared/clip_balance_30",
              "polypo/laion-CLIP-ViT-bigG-14-laion2B-39B-b160k",
              "kvriza8/clip-microscopy-30-epoch-sem_tags",
              "kvriza8/clip-microscopy-100-epoch-sem_tags",
              "kvriza8/clip-microscopy-500-epoch-sem",
              "sachin/tiny_clip",
              "deepghs/ccip",
              "mudra1710/clip-embeddings",
              "deepghs/ccip_onnx",
              "wangyi111/DeCUR",
              "xinyu1205/recognize-anything-plus-model",
              "JingyaoLi/MOODv2",
              "fashxp/zero-shot-image-classification",
              "pimcore/IEP__zero-shot-image-classification",
              "Ukawa/Fine-tunec-ViT-L-14",
              "Ukawa/Fine-tuned-ViT-L-14",
              "Yiyuan/CLIP-UniRepLKNet-L-laion5B-s10B-b75k",
              "pykale/MeDSLIP",
              "ramfais/edgevl_weights",
              "PetchP/clip-ViT-B-32-Petch",
              "ChrisXiao/EndoSAM",
              "ytaek-oh/fsc-clip",
              "PJMixers-Dev/microsoft_LLM2CLIP-Openai-L-14-336-Combined",
              "microsoft/LLM2CLIP-Llama3.2-1B-EVA02-L-14-336",
              "nolan4/modernBERT-base-CLIP",
              "google/siglip2-base-patch16-224-jax",
              "google/siglip2-base-patch16-256-jax",
              "google/siglip2-base-patch16-384-jax",
              "google/siglip2-base-patch16-512-jax",
              "google/siglip2-base-patch16-naflex-jax",
              "google/siglip2-base-patch32-256-jax",
              "google/siglip2-giant-opt-patch16-256-jax",
              "google/siglip2-giant-opt-patch16-384-jax",
              "google/siglip2-large-patch16-256-jax",
              "google/siglip2-large-patch16-384-jax",
              "google/siglip2-large-patch16-512-jax",
              "google/siglip2-so400m-patch14-224-jax",
              "google/siglip2-so400m-patch14-384-jax",
              "google/siglip2-so400m-patch16-256-jax",
              "google/siglip2-so400m-patch16-384-jax",
              "google/siglip2-so400m-patch16-512-jax",
              "google/siglip2-so400m-patch16-naflex-jax"
            ],
            "placeholder": "Select or type model name",
            "style": "IPY_MODEL_fad999be0e8c42c6bd97b89bcfbbbf1e",
            "value": "openai/clip-vit-large-patch14"
          }
        },
        "d06c437aa9b34fb7887191dd225687d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fad999be0e8c42c6bd97b89bcfbbbf1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "078ede06f0dc42279e8f6b9ee91b305f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Labels:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_fbb7ce25ad4446149c43ab785d975bb3",
            "placeholder": "Enter comma-separated labels",
            "style": "IPY_MODEL_b6c21ed304d547019eb340475e9f87df",
            "value": "screenshot of a newspaper article, a picture of a person, a picture of a politician, memes and computer-generated images"
          }
        },
        "fbb7ce25ad4446149c43ab785d975bb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "b6c21ed304d547019eb340475e9f87df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7da18e4cc390436482e4d8d3ec05279c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "copy",
              "move"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Operation:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_31eda5bd85da4074aa888440119cb963",
            "style": "IPY_MODEL_202790b5562f4ebbbe3ebfecb831f505"
          }
        },
        "31eda5bd85da4074aa888440119cb963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "202790b5562f4ebbbe3ebfecb831f505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dd3b0dcca834bd2be30d7d817d8beff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Start Classification",
            "disabled": false,
            "icon": "play",
            "layout": "IPY_MODEL_ae4823a5a2be49bb873d957d93a60054",
            "style": "IPY_MODEL_a8f3bea7ec56468f8ce3fc81137452bc",
            "tooltip": "Click to start image classification"
          }
        },
        "ae4823a5a2be49bb873d957d93a60054": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f3bea7ec56468f8ce3fc81137452bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}